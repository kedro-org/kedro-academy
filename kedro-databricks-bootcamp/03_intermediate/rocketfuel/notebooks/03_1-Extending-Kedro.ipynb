{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437cc40f",
   "metadata": {},
   "source": [
    "# 03.1 - Extending Kedro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a718b3d",
   "metadata": {},
   "source": [
    "## Custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a931e",
   "metadata": {},
   "source": [
    "You have a large collection of datasets maintained by the core team in the [`kedro-datasets`](https://pypi.org/package/kedro-datasets) package. Some of them have been used in this bootcamp already, for example `pandas.ExcelDataset`, `spark.SparkDataset`, or `databricks.ManagedTableDataset`.\n",
    "\n",
    "And yet, sometimes you will want to implement your own custom dataset, either because of custom data formats or Python libraries or because you want extra features not available in the official dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f87702",
   "metadata": {},
   "source": [
    "For example, let's create a simple Delta Table dataset backed by Spark, compatible with the existing Delta Tables you have declared in your Kedro catalog.\n",
    "\n",
    "Usage will look like this:\n",
    "\n",
    "```yaml\n",
    "companies:\n",
    "  type: rocketfuel.datasets.SimpleDeltaTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: companies\n",
    "  write_mode: overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b7480",
   "metadata": {},
   "source": [
    "Notice that the `type` is set to `rocketfuel.datasets.SimpleDeltaTableDataset`. That is the class that Kedro will try to `import`.\n",
    "\n",
    "As such, start by creating a `rockefuel/src/rocketfuel/datasets.py` file, containing a a subclass of `kedro.io.AbstractDataset`, with an `__init__` method reflecting the properties of the YAML entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/rocketfuel/datasets.py\n",
    "from kedro.io import AbstractDataset\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "class SimpleDeltaTableDataset(AbstractDataset[DataFrame, DataFrame]):\n",
    "    def __init__(\n",
    "        self, catalog: str, database: str, table: str, write_mode: str = \"overwrite\"\n",
    "    ):\n",
    "        self._catalog = catalog\n",
    "        self._schema = database\n",
    "        self._table = table\n",
    "\n",
    "        if write_mode != \"overwrite\":\n",
    "            raise NotImplementedError(\"Only overwrite mode is supported\")\n",
    "        self._write_mode = write_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac4078",
   "metadata": {},
   "source": [
    "Next, implement the required abstract methods:\n",
    "\n",
    "- `load`\n",
    "- `save`\n",
    "- `_describe`\n",
    "\n",
    "The `load` method will be invoked when the dataset is an _input_ of a node in a pipeline, and is expected to return the data that the node will use. The `save` method, on the other hand, will be invoked when the dataset is an _output_ of a node, and is expected to receive the data that the node produced.\n",
    "\n",
    "```python\n",
    "pipeline([\n",
    "    node(\n",
    "        func=_noop,\n",
    "        inputs=\"companies_raw\",  # .load() will be called, return value will be passed to noop(df)\n",
    "        outputs=\"companies\",  # the return value of noop(df) will be used to call .save(data)\n",
    "        name=\"companies_load_node\",\n",
    "    ),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/rocketfuel/datasets.py\n",
    "import typing as t\n",
    "\n",
    "from kedro.io import AbstractDataset\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "\n",
    "class SimpleDeltaTableDataset(AbstractDataset[DataFrame, DataFrame]):\n",
    "    def __init__(\n",
    "        self, catalog: str, database: str, table: str, write_mode: str = \"overwrite\"\n",
    "    ):\n",
    "        self._catalog = catalog\n",
    "        self._schema = database\n",
    "        self._table = table\n",
    "\n",
    "        if write_mode != \"overwrite\":\n",
    "            raise NotImplementedError(\"Only overwrite mode is supported\")\n",
    "        self._write_mode = write_mode\n",
    "\n",
    "        self._full_table_location = (\n",
    "            f\"`{self._catalog}`.`{self._schema}`.`{self._table}`\"\n",
    "        )\n",
    "\n",
    "    def load(self) -> DataFrame:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        data = spark.table(self._full_table_location)\n",
    "        return data\n",
    "\n",
    "    def save(self, data: DataFrame) -> None:\n",
    "        writer = (\n",
    "            data.write.format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "        )\n",
    "        writer.saveAsTable(self._full_table_location)\n",
    "\n",
    "    def _describe(self) -> dict[str, t.Any]:\n",
    "        return {\n",
    "            \"catalog\": self._catalog,\n",
    "            \"database\": self._schema,\n",
    "            \"table\": self._table,\n",
    "            \"write_mode\": self._write_mode,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195fe6f",
   "metadata": {},
   "source": [
    "Next, adapt the `companies` dataset in the catalog to use the new dataset:\n",
    "\n",
    "```diff\n",
    " companies:\n",
    "-  type: databricks.ManagedTableDataset\n",
    "+  type: rocketfuel.datasets.SimpleDeltaTableDataset\n",
    "   catalog: ${_uc_catalog}\n",
    "   database: ${_uc_schema}\n",
    "   table: companies\n",
    "   write_mode: overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76ea46",
   "metadata": {},
   "source": [
    "And finally, bootstrap the project to verify that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa82293",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_kedro --env databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog._get_dataset(\"companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(catalog.load(\"companies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda9e8d",
   "metadata": {},
   "source": [
    "### Exercise NN\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9ffe0",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb856a",
   "metadata": {},
   "source": [
    "![Kedro lifecycle](kedro_run_lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb29df",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
