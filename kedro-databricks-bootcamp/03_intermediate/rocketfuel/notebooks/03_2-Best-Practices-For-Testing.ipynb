{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331bb177-22ba-48f3-a034-dcc2e780b9aa",
   "metadata": {},
   "source": [
    "# Test a Kedro project\n",
    "\n",
    "It is important to test our Kedro projects to validate and verify that our nodes and pipelines behave as we expect them to. In this section we look at some example tests for the rocketfuel project.\n",
    "\n",
    "This section explains the following:\n",
    "\n",
    "- How to test a Kedro node\n",
    "\n",
    "- How to test a Kedro pipeline\n",
    "\n",
    "- Testing best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ca335d-d6ca-460e-a188-0cc019b1e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940154d9-cce9-4165-ba3e-9de0d74fe29b",
   "metadata": {},
   "source": [
    "## Writing tests for Kedro nodes: Unit testing\n",
    "\n",
    "- Kedro expects node functions to be pure functions; a pure function is one whose output follows solely from its inputs, without any   observable side effects. \n",
    "- Testing these functions checks that a node will behave as expected - for a given set of input values, a node will produce the expected output. These tests are referred to as unit tests.\n",
    "\n",
    "Let us explore what this looks like in practice. Consider the node function split_data defined in the data science pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51934741-8b49-4c50-b0ba-5ef5fb72ed6d",
   "metadata": {},
   "source": [
    "Recommendation: https://docs.pytest.org/en/7.1.x/explanation/anatomy.html#anatomy-of-a-test\n",
    "1. Arrange -  prepare everything for our test\n",
    "2. Act - state-changing action that kicks off the behavior we want to test\n",
    "3. Assert - we look at that resulting state and check if it looks how weâ€™d expect after the dust has settled\n",
    "4. Cleanup - is where the test picks up after itself, so other tests arenâ€™t being accidentally influenced by it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90fbb4d-976e-42f1-8870-60c37eb090f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data: pd.DataFrame, parameters: Dict) -> Tuple:\n",
    "    \"\"\"Splits data into features and targets training and test sets.\n",
    "\n",
    "    Args:\n",
    "        data: Data containing features and target.\n",
    "        parameters: Parameters defined in parameters/data_science.yml.\n",
    "    Returns:\n",
    "        Split data.\n",
    "    \"\"\"\n",
    "    X = data[parameters[\"features\"]]\n",
    "    y = data[\"price\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da83dc80-58e9-4942-86af-506dd1b2f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test case\n",
    "def test_split_data():\n",
    "    # Arrange\n",
    "    dummy_data = pd.DataFrame(\n",
    "        {\n",
    "            \"engines\": [1, 2, 3],\n",
    "            \"crew\": [4, 5, 6],\n",
    "            \"passenger_capacity\": [5, 6, 7],\n",
    "            \"price\": [120, 290, 30],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dummy_parameters = {\n",
    "        \"model_options\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"random_state\": 3,\n",
    "            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Act\n",
    "    X_train, X_test, y_train, y_test = split_data(dummy_data, dummy_parameters[\"model_options\"])\n",
    "\n",
    "    # Assert\n",
    "    assert len(X_train) == 2\n",
    "    assert len(y_train) == 2\n",
    "    assert len(X_test) == 1\n",
    "    assert len(y_test) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14ce99e-051c-4ecc-9674-13aa792e687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a81841-114a-4fb7-98b4-ad47a441f65a",
   "metadata": {},
   "source": [
    "This test is an example of positive testing - it tests that a valid input produces the expected output. The inverse, testing that an \n",
    "invalid output will be appropriately rejected, is called negative testing and is equally as important.\n",
    "\n",
    "Using the same steps as above, we can write the following test to validate an error is thrown when price data is not available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09d31dd-64e9-47b1-8043-34a28bc1915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "def test_split_data_missing_price():\n",
    "    # Arrange\n",
    "    dummy_data = pd.DataFrame(\n",
    "        {\n",
    "            \"engines\": [1, 2, 3],\n",
    "            \"crew\": [4, 5, 6],\n",
    "            \"passenger_capacity\": [5, 6, 7],\n",
    "            # Note the missing price data\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dummy_parameters = {\n",
    "        \"model_options\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"random_state\": 3,\n",
    "            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with pytest.raises(KeyError) as e_info:\n",
    "        # Act\n",
    "        X_train, X_test, y_train, y_test = split_data(dummy_data, dummy_parameters[\"model_options\"])\n",
    "\n",
    "    # Assert\n",
    "    assert \"price\" in str(e_info.value) # checks that the error is about the missing price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19c9e1d-b742-4eb9-ba83-06501f53b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_split_data_missing_price()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2415f7b-2f12-4011-8708-00c3377aa362",
   "metadata": {},
   "source": [
    "## Writing tests for Kedro pipelines: Integration testing\n",
    "\n",
    "Writing tests for each node ensures each node will behave as expected when run individually. However, we must also consider how nodes in a pipeline interact with each other - this is called integration testing. Integration testing combines individual units as a group and checks whether they communicate, share data, and work together as expected. Let us look at this in practice.\n",
    "\n",
    "Consider the data science pipeline as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db48ee2-32c9-46e7-81de-3ec8ca3e43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.pipeline import Pipeline, node, pipeline\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import max_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(data: pd.DataFrame, parameters: dict) -> tuple:\n",
    "    \"\"\"Splits data into features and targets training and test sets.\n",
    "\n",
    "    Args:\n",
    "        data: Data containing features and target.\n",
    "        parameters: Parameters defined in parameters/data_science.yml.\n",
    "    Returns:\n",
    "        Split data.\n",
    "    \"\"\"\n",
    "    X = data[parameters[\"features\"]]\n",
    "    y = data[\"price\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def train_model(X_train: pd.DataFrame, y_train: pd.Series) -> LinearRegression:\n",
    "    \"\"\"Trains the linear regression model.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training data of independent features.\n",
    "        y_train: Training data for price.\n",
    "\n",
    "    Returns:\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    return regressor\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Calculates and logs the coefficient of determination.\n",
    "\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    me = max_error(y_test, y_pred)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Model has a coefficient R^2 of %.3f on test data.\", score)\n",
    "    return {\"r2_score\": score, \"mae\": mae, \"max_error\": me}\n",
    "\n",
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    return pipeline(\n",
    "        [\n",
    "            node(\n",
    "                func=split_data,\n",
    "                inputs=[\"model_input_table\", \"params:model_options\"],\n",
    "                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n",
    "                name=\"split_data_node\",\n",
    "            ),\n",
    "            node(\n",
    "                func=train_model,\n",
    "                inputs=[\"X_train\", \"y_train\"],\n",
    "                outputs=\"regressor\",\n",
    "                name=\"train_model_node\",\n",
    "            ),\n",
    "            node(\n",
    "                func=evaluate_model,\n",
    "                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n",
    "                outputs=None,\n",
    "                name=\"evaluate_model_node\",\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0f47c-3dce-4efc-b0e1-f39f0347f8c2",
   "metadata": {},
   "source": [
    "The pipeline takes a pandas DataFrame and dictionary of parameters as input, splits the data in accordance to the parameters, \n",
    "and uses it to train and evaluate a regression model. \n",
    "\n",
    "With an integration test, we can validate that this sequence of nodes runs as expected.\n",
    "\n",
    "As we did with our unit tests, we break this down into several steps:\n",
    "1. Arrange: Prepare the runner and its inputs pipeline and catalog, and any additional test setup.\n",
    "2. Act: Run the pipeline.\n",
    "3. Assert: Ensure a successful run message was logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d02c239-da9d-43b4-b757-e77b48162aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pipeline = create_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae44ae59-92d2-4cf4-b132-c984d1504f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.io import DataCatalog\n",
    "from kedro.runner import SequentialRunner\n",
    "\n",
    "def test_data_science_pipeline(caplog):    # Note: caplog is passed as an argument\n",
    "    # Arrange pipeline\n",
    "    pipeline = ds_pipeline\n",
    "\n",
    "    # Arrange data catalog\n",
    "    catalog = DataCatalog()\n",
    "\n",
    "    dummy_data = pd.DataFrame(\n",
    "        {\n",
    "            \"engines\": [1, 2, 3],\n",
    "            \"crew\": [4, 5, 6],\n",
    "            \"passenger_capacity\": [5, 6, 7],\n",
    "            \"price\": [120, 290, 30],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dummy_parameters = {\n",
    "        \"model_options\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"random_state\": 3,\n",
    "            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    catalog.add_feed_dict(\n",
    "        {\n",
    "            \"model_input_table\" : dummy_data,\n",
    "            \"params:model_options\": dummy_parameters[\"model_options\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Arrange the log testing setup\n",
    "    caplog.set_level(logging.DEBUG, logger=\"kedro\") # Ensure all logs produced by Kedro are captured\n",
    "    successful_run_msg = \"Pipeline execution completed successfully.\"\n",
    "\n",
    "    # Act\n",
    "    SequentialRunner().run(pipeline, catalog)\n",
    "\n",
    "    # Assert\n",
    "    assert successful_run_msg in caplog.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54d54104-93bf-4755-8d22-2ce75336ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_science_pipeline(caplog)\n",
    "# This does not work as caplog is a pytest fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9614b-7f6c-4432-b759-3ad008f33baa",
   "metadata": {},
   "source": [
    "## Testing best practices\n",
    "\n",
    "<b>1. Where to write your tests:</b> We recommend creating a tests directory within the root directory of your project\n",
    "\n",
    "    <pre><code>\n",
    "        \n",
    "    src\n",
    "    â”‚   ...\n",
    "    â””â”€â”€â”€spaceflights\n",
    "    â”‚   â””â”€â”€â”€pipelines\n",
    "    â”‚       â””â”€â”€â”€data_science\n",
    "    â”‚           â”‚   __init__.py\n",
    "    â”‚           â”‚   nodes.py\n",
    "    â”‚           â”‚   pipeline.py\n",
    "    â”‚\n",
    "    tests\n",
    "    |   ...\n",
    "    â””â”€â”€â”€pipelines\n",
    "    â”‚   â””â”€â”€â”€data_science\n",
    "    â”‚       â”‚   test_data_science_pipeline.py\n",
    "        \n",
    "    </code></pre>\n",
    "\n",
    "</br>\n",
    "\n",
    "<b>2. Using fixtures:</b> In our tests, we can see that dummy_data and dummy_parameters have been defined three times with (mostly) the same values. Instead, we can define these outside of our tests as pytest fixtures\n",
    "\n",
    "```python\n",
    "    \n",
    "    import pytest\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def dummy_data():\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"engines\": [1, 2, 3],\n",
    "                \"crew\": [4, 5, 6],\n",
    "                \"passenger_capacity\": [5, 6, 7],\n",
    "                \"price\": [120, 290, 30],\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def dummy_parameters():\n",
    "        parameters = {\n",
    "            \"model_options\": {\n",
    "                \"test_size\": 0.2,\n",
    "                \"random_state\": 3,\n",
    "                \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n",
    "            }\n",
    "        }\n",
    "        return parameters\n",
    "    \n",
    "```\n",
    "</br>\n",
    "\n",
    "    We can then access these through the test arguments.\n",
    "\n",
    "```python\n",
    "    def test_split_data(dummy_data, dummy_parameters):\n",
    "            ...\n",
    "```\n",
    "</br>\n",
    "\n",
    "<b> 3. Pipeline Slicing:</b> In the test `test_data_science_pipeline` we test the data science pipeline, as currently defined, can be run successfully. However, as pipelines are not static, this test is not robust. Instead we should be specific with how we define the pipeline to be tested; we do this by using pipeline slicing to specify the pipelineâ€™s start and end:\n",
    "\n",
    "```python\n",
    "    def test_data_science_pipeline(self):\n",
    "        # Arrange pipeline\n",
    "        pipeline = create_pipeline().from_nodes(\"split_data_node\").to_nodes(\"evaluate_model_node\")\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c975469-1040-40e8-8bac-8177a1e7dc38",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "\n",
    "1. Understand the `test_pipeline` file at - \n",
    "https://github.com/kedro-org/kedro-academy/blob/main/kedro-databricks-bootcamp/03_intermediate/rocketfuel/tests/pipelines/data_science/test_pipeline.py\n",
    "2. Try to run your test file following the doc - https://docs.kedro.org/en/stable/tutorial/test_a_project.html#run-your-tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db40ed8-51a2-4eb1-b418-30c8a4c66ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (demo_project)",
   "language": "python",
   "name": "kedro_demo_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
