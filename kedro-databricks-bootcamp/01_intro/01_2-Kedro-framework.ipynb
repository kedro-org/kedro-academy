{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87bd0101-f4bc-4e53-b87c-3875ef98fdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01.2 - Kedro framework\n",
    "\n",
    "<img src=\"../static/kedro-horizontal-color-on-light.png\" width=\"400\" alt=\"Kedro\">\n",
    "\n",
    "This notebook covers how to use the Kedro CLI to create and manage projects using the Kedro framework, which assembles the library components seen in [First steps with Kedro](./01_1-First%20Steps%20with%20Kedro.ipynb) in a standard way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1958397b-8a58-4504-b5cd-caa241012e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Starters\n",
    "\n",
    "A Kedro starter contains code in the form of a Cookiecutter template for a Kedro project. Using a starter is like using a pre-defined layout when creating a presentation or document.\n",
    "\n",
    "You can find [the official list of starters](https://docs.kedro.org/en/0.19.10/starters/starters.html#official-kedro-starters) in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis for the rest of this notebook will be the [`spaceflights-pandas`](https://github.com/kedro-org/kedro-starters/tree/0.19.10/spaceflights-pandas) starter, which is ideal for local execution. Later on in the bootcamp you will move on to the `spaceflights-pyspark` starter, which uses PySpark and is ready to be executed in Databricks.\n",
    "\n",
    "To use it, you will first need `kedro` installed. You can use `conda`, a separate virtual environment, or a Python workflow tool capable of managing global utilities, such as pipx or uv.\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro new --starter=spaceflights-pandas --name rocketfuel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The directory structure\n",
    "\n",
    "A typical Kedro project looks like this:\n",
    "\n",
    "```\n",
    "project-dir          # Parent directory of the template\n",
    "├── conf             # Project configuration files\n",
    "├── data             # Local project data (not committed to version control)\n",
    "├── docs             # Project documentation\n",
    "├── notebooks        # Project-related Jupyter notebooks (can be used for experimental code before moving the code to src)\n",
    "├── src              # Project source code\n",
    "├── tests            # Folder containing unit and integration tests\n",
    "├── .gitignore       # Hidden file that prevents staging of unnecessary files to `git`\n",
    "├── pyproject.toml   # Identifies the project root and contains configuration information\n",
    "├── README.md        # Project README\n",
    "├── requirements.txt # Project dependencies file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running pipelines\n",
    "\n",
    "You can use the Kedro CLI to run any pipeline that has been registered.\n",
    "\n",
    "To see the pipelines already registered, in your project, run\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro registry list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `kedro run` to execute the pipeline of your liking:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro run --pipeline data_processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after the `data_processing` pipeline is executed, you should be able to see the results in the appropriate local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd rocketfuel && ls data/02_intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environments\n",
    "\n",
    "A [configuration environment](https://docs.kedro.org/en/0.19.10/configuration/configuration_basics.html#configuration-environments) is a way of organising your configuration settings for different stages of your data pipeline. For example, you might have different settings for development, testing, and production environments.\n",
    "\n",
    "By default, Kedro projects have a `base` and a `local` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline creation\n",
    "\n",
    "To create a new pipeline, you can use the CLI:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro pipeline create PIPELINE_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Integration with VS Code\n",
    "\n",
    "Kedro has an official extension for VS Code, providing features like enhanced code navigation and autocompletion for seamless development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualisation with Kedro Viz\n",
    "\n",
    "You can use Kedro Viz to visualise your pipelines in 3 ways:\n",
    "\n",
    "1. Using the `NotebookVisualizer` (see 01-1 notebook)\n",
    "2. Using the VS Code integration https://docs.kedro.org/en/stable/development/set_up_vscode.html#visualise-the-pipeline-with-kedro-viz\n",
    "3. Launching the Kedro Viz web application on the command line:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro viz run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Register two new pipelines in the pipeline registry called `train` and `inference` that use the appropriate nodes from the `data_science` pipeline.\n",
    "\n",
    "Visualise them in Kedro Viz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remote data paths in the catalog\n",
    "\n",
    "Kedro datasets can be virtually anything: database connections, REST APIs, and of course files.\n",
    "\n",
    "These files can be referenced by using remote filepaths, not just local ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We will showcase how an example with DBFS would work, but notice that Storing and accessing data using DBFS root or DBFS mounts is a deprecated pattern and not recommended by Databricks._\n",
    "\n",
    "For example, imagine your Databricks DBFS contains an example `/Shared/patients.csv`:\n",
    "\n",
    "```python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "assert w.dbfs.exists(\"/Shared/patients.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most [official Kedro datasets](https://docs.kedro.org/projects/kedro-datasets/en/kedro-datasets-7.0.0/api/kedro_datasets.html) use [`fsspec`](https://filesystem-spec.readthedocs.io/),\n",
    "a Python library that allows users to easily specify remote filepaths\n",
    "with [lots of different cloud filesystems](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations),\n",
    "including DBFS.\n",
    "\n",
    "```python\n",
    "import fsspec\n",
    "import os\n",
    "\n",
    "dbfs = fsspec.filesystem(\n",
    "    \"dbfs\", instance=os.environ[\"DATABRICKS_INSTANCE\"], token=os.environ[\"DATABRICKS_TOKEN\"],\n",
    ")\n",
    "files = dbfs.ls(\"/Shared/\")\n",
    "print(files)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load this file from Kedro, you will need two things:\n",
    "\n",
    "- A proper way of specifying the credentials, and\n",
    "- A URL with the appropriate protocol specifier\n",
    "\n",
    "For credentials, Kedro allows you to use environment variables, but it will force you to specify the desired properties.\n",
    "\n",
    "The default `.gitignore` has some protections to avoid accidentally committing them to version control.\n",
    "\n",
    "In this case, you could use the `local` environment:\n",
    "\n",
    "```yaml\n",
    "# conf/local/credentials.yml\n",
    "databricks:\n",
    "  instance: ${oc.env:DATABRICKS_INSTANCE}\n",
    "  token: ${oc.env:DATABRICKS_TOKEN}\n",
    "```\n",
    "\n",
    "And finally, by specifying a `dbfs://` URL and the proper `credentials` key, you can define an appropriate dataset in the Kedro catalog:\n",
    "\n",
    "```yaml\n",
    "# conf/local/catalog.yml\n",
    "patients:\n",
    "  type: pandas.CSVDataset\n",
    "  filepath: dbfs:///Shared/patients.csv\n",
    "  credentials: databricks\n",
    "```\n",
    "\n",
    "Notice the triple slash `///`, the first two correspond to the protocol `dbfs://` and the third one marks an absolute path `/Shared`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the training will favour using the Unity Catalog rather than direct access to DBFS, as recommended by Databricks.\n",
    "\n",
    "However, this shows that you can use this pattern for direct cloud storage access (`s3://`, `abfs://`), files in websites (`https://`), and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "assert w.dbfs.exists(\"/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/unstructured/companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pyspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/unstructured/companies.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "pyspark.read.csv(\"/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/unstructured/companies.csv\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "|  _c0|           _c1|                 _c2|              _c3|          _c4|\n",
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "|   id|company_rating|    company_location|total_fleet_count|iata_approved|\n",
      "| 3888|          100%|         Isle of Man|              1.0|            f|\n",
      "|46728|          100%|                NULL|              1.0|            f|\n",
      "|34618|           38%|         Isle of Man|              1.0|            f|\n",
      "|28619|          100%|Bosnia and Herzeg...|              1.0|            f|\n",
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/unstructured/companies.csv\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "|  _c0|           _c1|                 _c2|              _c3|          _c4|\n",
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "|   id|company_rating|    company_location|total_fleet_count|iata_approved|\n",
      "| 3888|          100%|         Isle of Man|              1.0|            f|\n",
      "|46728|          100%|                NULL|              1.0|            f|\n",
      "|34618|           38%|         Isle of Man|              1.0|            f|\n",
      "|28619|          100%|Bosnia and Herzeg...|              1.0|            f|\n",
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"dbfs:/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/unstructured/companies.csv\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "|  _c0|           _c1|                 _c2|              _c3|          _c4|\n",
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "|   id|company_rating|    company_location|total_fleet_count|iata_approved|\n",
      "| 3888|          100%|         Isle of Man|              1.0|            f|\n",
      "|46728|          100%|                NULL|              1.0|            f|\n",
      "|34618|           38%|         Isle of Man|              1.0|            f|\n",
      "|28619|          100%|Bosnia and Herzeg...|              1.0|            f|\n",
      "+-----+--------------+--------------------+-----------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"dbfs:///Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/unstructured/companies.csv\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01_2-Kedro-framework",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
