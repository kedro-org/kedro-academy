{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87bd0101-f4bc-4e53-b87c-3875ef98fdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01.2 - Kedro framework\n",
    "\n",
    "<img src=\"../static/kedro-horizontal-color-on-light.png\" width=\"400\" alt=\"Kedro\">\n",
    "\n",
    "This notebook covers how to use the Kedro CLI to create and manage projects using the Kedro framework, which assembles the library components seen in [First steps with Kedro](./01_1-First%20Steps%20with%20Kedro.ipynb) in a standard way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1958397b-8a58-4504-b5cd-caa241012e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Starters\n",
    "\n",
    "A Kedro starter contains code in the form of a Cookiecutter template for a Kedro project. Using a starter is like using a pre-defined layout when creating a presentation or document.\n",
    "\n",
    "You can find [the official list of starters](https://docs.kedro.org/en/0.19.10/starters/starters.html#official-kedro-starters) in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis for the rest of this notebook will be the [`spaceflights-pandas`](https://github.com/kedro-org/kedro-starters/tree/0.19.10/spaceflights-pandas) starter, which is ideal for local execution. Later on in the bootcamp you will move on to the `spaceflights-pyspark` starter, which uses PySpark and is ready to be executed in Databricks.\n",
    "\n",
    "To use it, you will first need `kedro` installed. You can use `conda`, a separate virtual environment, or a Python workflow tool capable of managing global utilities, such as pipx or uv.\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro new --starter=spaceflights-pandas --name rocketfuel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The directory structure\n",
    "\n",
    "A typical Kedro project looks like this:\n",
    "\n",
    "```\n",
    "project-dir          # Parent directory of the template\n",
    "├── conf             # Project configuration files\n",
    "├── data             # Local project data (not committed to version control)\n",
    "├── docs             # Project documentation\n",
    "├── notebooks        # Project-related Jupyter notebooks (can be used for experimental code before moving the code to src)\n",
    "├── src              # Project source code\n",
    "├── tests            # Folder containing unit and integration tests\n",
    "├── .gitignore       # Hidden file that prevents staging of unnecessary files to `git`\n",
    "├── pyproject.toml   # Identifies the project root and contains configuration information\n",
    "├── README.md        # Project README\n",
    "├── requirements.txt # Project dependencies file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running pipelines\n",
    "\n",
    "You can use the Kedro CLI to run any pipeline that has been registered.\n",
    "\n",
    "To see the pipelines already registered, in your project, run\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro registry list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `kedro run` to execute the pipeline of your liking:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro run --pipeline data_processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after the `data_processing` pipeline is executed, you should be able to see the results in the appropriate local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd rocketfuel && ls data/02_intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environments\n",
    "\n",
    "A [configuration environment](https://docs.kedro.org/en/0.19.10/configuration/configuration_basics.html#configuration-environments) is a way of organising your configuration settings for different stages of your data pipeline. For example, you might have different settings for development, testing, and production environments.\n",
    "\n",
    "By default, Kedro projects have a `base` and a `local` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline creation\n",
    "\n",
    "To create a new pipeline, you can use the CLI:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro pipeline create PIPELINE_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Integration with VS Code\n",
    "\n",
    "Kedro has an official extension for VS Code, providing features like enhanced code navigation and autocompletion for seamless development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualisation with Kedro Viz\n",
    "\n",
    "You can use Kedro Viz to visualise your pipelines in 3 ways:\n",
    "\n",
    "1. Using the `NotebookVisualizer` (see 01-1 notebook)\n",
    "2. Using the VS Code integration https://docs.kedro.org/en/stable/development/set_up_vscode.html#visualise-the-pipeline-with-kedro-viz\n",
    "3. Launching the Kedro Viz web application on the command line:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro viz run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Register two new pipelines in the pipeline registry called `train` and `inference` that use the appropriate nodes from the `data_science` pipeline.\n",
    "\n",
    "Visualise them in Kedro Viz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remote data paths in the catalog\n",
    "\n",
    "Kedro datasets can be virtually anything: database connections, REST APIs, and of course files.\n",
    "\n",
    "These files can be referenced by using remote filepaths, not just local ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most [official Kedro datasets](https://docs.kedro.org/projects/kedro-datasets/en/kedro-datasets-7.0.0/api/kedro_datasets.html) use [`fsspec`](https://filesystem-spec.readthedocs.io/),\n",
    "a Python library that allows users to easily specify remote filepaths\n",
    "with [lots of different cloud filesystems](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations),\n",
    "including DBFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this dataset definition:\n",
    "\n",
    "```\n",
    "companies:\n",
    "  type: spark.SparkDataset\n",
    "  filepath: dbfs:/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/bronze/companies.csv\n",
    "  file_format: csv\n",
    "```\n",
    "\n",
    "Provided that `databricks-connect` is properly configured and the data available in the right volume (see instructions for Day 02), this will trivially work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hdfs s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.io import DataCatalog\n",
    "\n",
    "catalog_remote_spark = DataCatalog.from_config({\n",
    "    \"companies\": {\n",
    "        \"type\": \"spark.SparkDataset\",\n",
    "        \"filepath\": \"dbfs:/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/bronze/companies.csv\",\n",
    "        \"file_format\": \"csv\",\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_remote_spark.load(\"companies\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note**: This example uses native PySpark reading capabilities. fsspec uses the DBFS API, which is not compatible with Unity Catalog Volumes https://github.com/fsspec/filesystem_spec/issues/1656_"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01_2-Kedro-framework",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
