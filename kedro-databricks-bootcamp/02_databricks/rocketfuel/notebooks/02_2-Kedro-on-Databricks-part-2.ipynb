{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19af4bc9-b683-4210-b429-6bc8be11fcd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02.2 - Kedro on Databricks, part 2\n",
    "\n",
    "_Notes: This notebook is supposed to be run locally from VS Code, all with Databricks Connect_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ef80ca-69c2-46e5-8a5f-c00a9fe45c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kedro and Databricks Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Databricks Connect** is a client library that allows you to run Spark code locally on your machine while connecting to a remote Databricks cluster for computation. It essentially lets you develop and execute Spark applications from your local IDE or notebook environment, but the actual processing happens on the Databricks cluster.\n",
    "\n",
    "The **Databricks extension for Visual Studio Code** has several interesting features for connecting to Databricks from VS Code and perform actions sach us deploying and running Databricks Asset Bundles, manage clusters, and easily set up **Databricks Connect**.\n",
    "\n",
    "Therefore, the two are the perfect companion for developing Kedro projects on VS Code, since you can develop on your IDE while using Databricks compute.\n",
    "\n",
    "Follow the official documentation to\n",
    "\n",
    "1. [Install the Databricks extension for VS Code](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/install)\n",
    "2. [Configure the appropriate cluster](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/configure)\n",
    "3. [Install Databricks Connect](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/databricks-connect)\n",
    "\n",
    "_Note: `databricks-connect` provides its own `pyspark` top-level module, and [pip doesn't check for conflicting packages](https://github.com/pypa/pip/issues/4625), so make sure you don't have a [conflicting `pyspark` installation](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect/python/troubleshooting#conflicting-pyspark-installations)!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the needed requirements again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../../../requirements.in\n",
    "%pip install hdfs s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uninstall dependencies that conflict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y kedro-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Kedro ipython extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your project now contains the databricks configuration created in the previous notebook. Add it into your `base/catalog.yml` file. The content should be as follows:\n",
    "\n",
    "```yaml\n",
    "_uc_catalog: #your catalog location\n",
    "_uc_schema: #your schema location\n",
    "\n",
    "companies_raw:\n",
    "  type: spark.SparkDataset\n",
    "  filepath: /Volumes/${_uc_catalog}/${_uc_schema}/bronze/companies.csv\n",
    "  file_format: csv\n",
    "  load_args:\n",
    "    header: True\n",
    "    inferSchema: True\n",
    "\n",
    "reviews_raw:\n",
    "  type: spark.SparkDataset\n",
    "  filepath: /Volumes/${_uc_catalog}/${_uc_schema}/bronze/reviews.csv\n",
    "  file_format: csv\n",
    "  load_args:\n",
    "    header: True\n",
    "    inferSchema: True\n",
    "\n",
    "companies:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: companies\n",
    "  write_mode: overwrite\n",
    "\n",
    "reviews:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: reviews\n",
    "  write_mode: overwrite\n",
    "\n",
    "shuttles:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: shuttles\n",
    "  write_mode: overwrite\n",
    "\n",
    "preprocessed_companies:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: preprocessed_companies\n",
    "  write_mode: overwrite\n",
    "\n",
    "preprocessed_shuttles:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: preprocessed_shuttles\n",
    "  write_mode: overwrite\n",
    "\n",
    "preprocessed_reviews:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: preprocessed_reviews\n",
    "  write_mode: overwrite\n",
    "\n",
    "model_input_table:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: model_input_table\n",
    "  write_mode: overwrite\n",
    "\n",
    "regressor:\n",
    "  type: pickle.PickleDataset\n",
    "  filepath: data/06_models/regressor.pickle\n",
    "  versioned: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%reload_kedro ../../rocketfuel"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how data is loaded as a PySpark DataFrame, directly from Databricks Unity Catalog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog._get_dataset(\"companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(catalog.load(\"companies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Codify the logic of the dummy `load_data` pipeline inside the project and run it locally through `databricks-connect`. For that:\n",
    "\n",
    "- Create the pipeline in your local development environment.\n",
    "- Run ```%reload_kedro``` to reload the Kedro project.\n",
    "- Try to execute the `load_data` pipeline from the VS Code notebook.\n",
    "- Iterate until it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%reload_kedro ../../rocketfuel"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below command let's you execute the `load_data` pipeline\n",
    "session.run(\"load_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07630ef1-c5c6-4396-b841-f6973b2f3123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Integration with Databricks MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef08d133-d23c-4243-a273-9c890221900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log Kedro runs as MLflow experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "635550a0-4476-4477-a71d-935f5a3d7e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are 2 types of MLflow experiments in Databricks:\n",
    "- **Workspace** experiments are not associated with any notebook, and any notebook can log a run to these experiments by using the experiment ID or the experiment name. _They cannot be created inside Git folders._\n",
    "- **Notebook** experiments are associated with a specific notebook. _They are note checked into source control_.\n",
    "\n",
    "Therefore, for personal experimentation **notebook** experiments are more appropriate, and for collaboration **workspace** experiments can be created in a regular workspace folder outside of Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you will be running this notebook locally using Databricks Connect, creating a **workspace** experiment will be more flexible. First, create the appropriate parent directory using the Databricks SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "current_user = w.current_user.me()\n",
    "home_dir = f\"/Users/{current_user.user_name}\"\n",
    "home_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need a Databricks token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Call `databricks configure` on the CLI instead of setting the environment variables here?\n",
    "#\n",
    "# import os\n",
    "#\n",
    "# os.environ[\"DATABRICKS_INSTANCE\"] = w.config.host\n",
    "# Do NOT commit this to version control!\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, verify that everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# This workaround is needed with serverless compute, see official answer at\n",
    "# https://community.databricks.com/t5/machine-learning/using-datbricks-connect-with-serverless-compute-and-mlflow/m-p/97604#M3764\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = (\n",
    "    lambda: \"databricks-uc\"\n",
    ")\n",
    "\n",
    "experiment_path = f\"{home_dir}/02_2-kedro-on-databricks\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow is the perfect companion for Kedro projects, thanks to the `kedro-mlflow` community plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d25c96-e66a-4a90-9c5d-087a9737a796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "%pip install kedro-mlflow"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd745b5-e7e3-4f02-8dad-738d19e27eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`kedro-mlflow` can take [configuration](https://kedro-mlflow.readthedocs.io/en/0.14.4/source/03_experiment_tracking/01_experiment_tracking/01_configuration.html) from `conf/<environment>/mlflow.yml`, which can be used to configure the experiment name.\n",
    "\n",
    "To this end, let's add some OmegaConf syntax to `mlflow.yml` so that the experiment name can be specified from the outside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3bf303-18a8-4314-b5ed-74f2def4eea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ../conf/local/mlflow.yml\n",
    "server:\n",
    "  mlflow_tracking_uri: databricks\n",
    "\n",
    "tracking:\n",
    "  experiment:\n",
    "    name: ${runtime_params:mlflow_experiment_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "712b00a6-6e06-46b2-b383-fc38bed5ed88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now you can pass the experiment name as a runtime parameter:\n",
    "\n",
    "_Note: Extra params cannot contain spaces when passed to `%reload_kedro`, see [this issue](https://github.com/kedro-org/kedro/issues/4813)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b869d0ed-84a1-48a2-9ca0-e303c14e09ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_kedro --params mlflow_experiment_name=$experiment_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd9ed83-fe75-40c4-97a1-bf4e48c75605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, every time a Kedro pipeline is run, it's logged as al MLflow run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164eeb73-3ca4-4358-9537-e9118b22bd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "session.run(\"data_processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ebedd2-1158-467c-98a6-da60d36b8ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![MLflow run corresponding to a Kedro run on Databricks](./kedro-databricks-mlflow-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Make the necessary changes to the project so that you can run a Kedro pipeline and log the results as a MLflow experiment from the CLI:\n",
    "\n",
    "```bash\n",
    "(.venv) $ kedro run -p data_processing --params mlflow_experiment_name=/Users/juan_luis_cano@mckinsey.com/02_2-kedro-on-databricks\n",
    "```\n",
    "\n",
    "_Note: To run locally you might need to `export DATABRICKS_SERVERLESS_COMPUTE_ID=auto`, see https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect/cluster-config#configure-a-connection-to-serverless-compute_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register models using the Databricks Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering models on the Unity Catalog from Kedro pipelines is trivial.\n",
    "\n",
    "Adjust the `regressor` dataset:\n",
    "\n",
    "```diff\n",
    " regressor:\n",
    "-  type: pickle.PickleDataset\n",
    "-  filepath: data/06_models/regressor.pickle\n",
    "-  versioned: true\n",
    "+  type: kedro_mlflow.io.models.MlflowModelTrackingDataset\n",
    "+  flavor: mlflow.sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the `data_science` pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_kedro --params mlflow_experiment_name=$experiment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(\"data_science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model registry](kedro-databricks-mlflow-model-registry.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5924746010652874,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_2-Kedro-on-Databricks-part-2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
