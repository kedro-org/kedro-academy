{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19af4bc9-b683-4210-b429-6bc8be11fcd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02.2 - Kedro on Databricks, part 2\n",
    "\n",
    "_Notes: This notebook is supposed to be run locally from VS Code, all with Databricks Connect_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ef80ca-69c2-46e5-8a5f-c00a9fe45c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kedro and Databricks Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Databricks Connect** is a client library that allows you to run Spark code locally on your machine while connecting to a remote Databricks cluster for computation. It essentially lets you develop and execute Spark applications from your local IDE or notebook environment, but the actual processing happens on the Databricks cluster.\n",
    "\n",
    "The **Databricks extension for Visual Studio Code** has several interesting features for connecting to Databricks from VS Code and perform actions sach us deploying and running Databricks Asset Bundles, manage clusters, and easily set up **Databricks Connect**.\n",
    "\n",
    "Therefore, the two are the perfect companion for developing Kedro projects on VS Code, since you can develop on your IDE while using Databricks compute.\n",
    "\n",
    "Follow the official documentation to\n",
    "\n",
    "1. [Install the Databricks extension for VS Code](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/install)\n",
    "2. [Configure the appropriate cluster](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/configure)\n",
    "3. [Install Databricks Connect](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/databricks-connect)\n",
    "\n",
    "_Note: `databricks-connect` provides its own `pyspark` top-level module, and [pip doesn't check for conflicting packages](https://github.com/pypa/pip/issues/4625), so make sure you don't have a [conflicting `pyspark` installation](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect/python/troubleshooting#conflicting-pyspark-installations)!_"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install the needed requirements again"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ../../rocketfuel\n",
    "# or if the above doesn't work: %pip install -r ../../../requirements.in\n",
    "%pip install hdfs s3fs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Uninstall dependencies that conflict:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip uninstall -y kedro-mlflow"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the Kedro ipython extension."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "/Ensure your project now contains the databricks configuration created in the previous notebook. The content should be as follows:\n",
    "\n",
    "```yaml\n",
    "_uc_catalog: #your catalog location\n",
    "_uc_schema: #your schema location\n",
    "\n",
    "companies_raw:\n",
    "  type: spark.SparkDataset\n",
    "  filepath: /Volumes/${_uc_catalog}/${_uc_schema}/bronze/companies.csv\n",
    "  file_format: csv\n",
    "  load_args:\n",
    "    header: True\n",
    "    inferSchema: True\n",
    "\n",
    "reviews_raw:\n",
    "  type: spark.SparkDataset\n",
    "  filepath: /Volumes/${_uc_catalog}/${_uc_schema}/bronze/reviews.csv\n",
    "  file_format: csv\n",
    "  load_args:\n",
    "    header: True\n",
    "    inferSchema: True\n",
    "\n",
    "shuttles_raw:\n",
    "  type: pandas.ExcelDataset\n",
    "  filepath: /data/01_raw/shuttles.xlsx #There are issues loading this from the workspace, so we use a local file.\n",
    "  load_args:\n",
    "    engine: openpyxl\n",
    "\n",
    "companies:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: companies\n",
    "  write_mode: overwrite\n",
    "\n",
    "reviews:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: reviews\n",
    "  write_mode: overwrite\n",
    "\n",
    "shuttles:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: shuttles\n",
    "  write_mode: overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%reload_kedro ../../rocketfuel --env databricks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how data is loaded as a PySpark DataFrame, directly from Databricks Unity Catalog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog._get_dataset(\"companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(catalog.load(\"companies\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 1\n",
    "\n",
    "Codify the logic of the dummy `load_data` pipeline inside the project and sync it to the Databricks instance through `databricks-connect`. For that:\n",
    "\n",
    "- Create the pipeline in your local development environment.\n",
    "- Sync the changes to Databricks. This should happen automatically when you save a file.\n",
    "- Run ```%reload_kedro``` to reload the Kedro project in Databricks.\n",
    "- Try to execute the `load_data` pipeline from the Databricks notebook.\n",
    "- Iterate until it works.\n",
    "\n",
    "!Note: The existing pipelines `data_processing` and `data_science` assume they're run locally and don't reference the new dataset definitions yet. You'll need to update them to use the datasets in the Unity Catalog instead of the local references."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%reload_kedro ../../rocketfuel --env databricks"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The below command let's you execute the `load_data` pipeline\n",
    "session.run(\"load_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dacd1e24-4302-4ae7-8064-49151c05d86d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Namespaces for pipeline grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namespaces allow you to group nodes, ensuring clear dependencies and separation within a pipeline while maintaining a consistent structure. Like with pipelines or tags, you can enable selective execution using namespaces, and you cannot run more than one namespace simultaneously — Kedro allows executing one namespace at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining namespace at Pipeline-level: When applying a namespace at the pipeline level, Kedro automatically renames all inputs, outputs, and parameters within that pipeline. You will need to update your catalog accordingly. If you don’t want to change the names of your inputs, outputs, or parameters with the `namespace_name`. prefix while using a namespace, you should list these objects inside the corresponding parameters of the `pipeline()` creation function. For example:\n",
    "\n",
    "```python\n",
    "return pipeline(\n",
    "    base_pipeline,\n",
    "    namespace = \"new_namespaced_pipeline\", # With that namespace, \"new_namespaced_pipeline\" prefix will be added to inputs, outputs, params, and node names\n",
    "    inputs={\"the_original_input_name\"}, # Inputs remain the same, without namespace prefix\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namespaces allow you to group your nodes and pipelines more efficiently in deployment, for example, when running your pipeline as a Databricks Job or Asset Bundle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a namespace to a Kedro pipeline, you can use the `namespace` argument in the `pipeline()` function. This argument accepts a string that will be used as a prefix for all nodes, inputs, outputs, and parameters within that pipeline. Note that you'll have to update your catalog accordingly, as Kedro expects that all inputs, outputs, and parameters within that pipeline include the namespace prefix. \n",
    "\n",
    "For example, to add a namespace to our `data_science` pipeline you'll have to modifify the code to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_science/pipeline.py\n",
    "\n",
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    return pipeline(\n",
    "        [\n",
    "            node(\n",
    "                func=split_data,\n",
    "                inputs=[\"model_input_table@pandas\", \"params:model_options\"],\n",
    "                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n",
    "                name=\"split_data_node\",\n",
    "            ),\n",
    "            node(\n",
    "                func=train_model,\n",
    "                inputs=[\"X_train\", \"y_train\"],\n",
    "                outputs=\"regressor\",\n",
    "                name=\"train_model_node\",\n",
    "            ),\n",
    "            node(\n",
    "                func=evaluate_model,\n",
    "                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n",
    "                outputs=None,\n",
    "                name=\"evaluate_model_node\",\n",
    "            ),\n",
    "        ], namespace=\"ds\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update catalog.yml to add namespace prefixes to relevant datasets\n",
    "\n",
    "```yaml\n",
    "ds.model_input_table@pandas:\n",
    "  type: pandas.ParquetDataset\n",
    "  filepath: data/03_primary/model_input_table.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters.yml to add namespace prefixes to relevant parameters\n",
    "\n",
    "```yaml\n",
    "ds.model_options:\n",
    "  test_size: 0.2\n",
    "  random_state: 3\n",
    "  features:\n",
    "    - engines\n",
    "    - passenger_capacity\n",
    "    - crew\n",
    "    - d_check_complete\n",
    "    - moon_clearance_complete\n",
    "    - iata_approved\n",
    "    - company_rating\n",
    "    - review_scores_rating\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07630ef1-c5c6-4396-b841-f6973b2f3123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Integration with Databricks MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef08d133-d23c-4243-a273-9c890221900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log Kedro runs as MLflow experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "635550a0-4476-4477-a71d-935f5a3d7e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are 2 types of MLflow experiments in Databricks:\n",
    "- **Workspace** experiments are not associated with any notebook, and any notebook can log a run to these experiments by using the experiment ID or the experiment name. _They cannot be created inside Git folders._\n",
    "- **Notebook** experiments are associated with a specific notebook. _They are note checked into source control_.\n",
    "\n",
    "Therefore, for personal experimentation **notebook** experiments are more appropriate, and for collaboration **workspace** experiments can be created in a regular workspace folder outside of Git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc97c7f-58d7-417e-8339-93e53c6a1e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "uv pip install ./rocketfuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ea8af0-ddd7-4704-893f-79030c67c6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b1c481-8869-46a4-9556-0fafc9b672d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Kedro runs can be logged in MLflow using the [`kedro-mlflow`](https://kedro-mlflow.readthedocs.io/) community plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d25c96-e66a-4a90-9c5d-087a9737a796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "uv pip install kedro-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd745b5-e7e3-4f02-8dad-738d19e27eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`kedro-mlflow` can take [configuration](https://kedro-mlflow.readthedocs.io/en/0.14.4/source/03_experiment_tracking/01_experiment_tracking/01_configuration.html) from `conf/<environment>/mlflow.yml`, which can be used to configure the experiment name:\n",
    "\n",
    "- For **notebook** experiments, you have to configure the experiment name to match the full path of the notebook.\n",
    "- For **workspace** experiments, the experiment name would be the full path to the experiments folder in the workspace.\n",
    "\n",
    "To this end, let's add some OmegaConf syntax to `mlflow.yml` so that the experiment name can be specified from the outside, while taking a default value if not present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3bf303-18a8-4314-b5ed-74f2def4eea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile rocketfuel/conf/databricks/mlflow.yml\n",
    "tracking:\n",
    "  experiment:\n",
    "    name: ${runtime_params:mlflow_experiment_name, ${kedro_root:}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f797c8-4da5-48ed-812e-ac6b3f63b8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's try to set up a **notebook** experiment. For this, extract the notebook path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285d2a47-4c2d-46e2-a020-38026dcd4288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "712b00a6-6e06-46b2-b383-fc38bed5ed88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And pass that as a runtime parameter to specify the experiment name:\n",
    "\n",
    "_Note: Extra params cannot contain spaces when passed to `%reload_kedro`, see [this issue](https://github.com/kedro-org/kedro/issues/4813)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b869d0ed-84a1-48a2-9ca0-e303c14e09ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_kedro rocketfuel --env databricks --params mlflow_experiment_name=$notebook_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd9ed83-fe75-40c4-97a1-bf4e48c75605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, every time a Kedro pipeline is run, it's logged as al MLflow run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164eeb73-3ca4-4358-9537-e9118b22bd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "session.run(\"load_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ebedd2-1158-467c-98a6-da60d36b8ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![MLflow run corresponding to a Kedro run on Databricks](./kedro-databricks-mlflow-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356e8174-e503-4ff2-ab1f-5edf8b8303ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log artifacts as MLflow models in the Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96631b7-582f-4217-a120-d292ff7d6ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20ae711-0b76-435d-a3ff-3d1903afe48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test code\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Train a sklearn model on the iris dataset\n",
    "X, y = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "clf = RandomForestClassifier(max_depth=7)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Note that the UC model name follows the pattern\n",
    "# <catalog_name>.<schema_name>.<model_name>, corresponding to\n",
    "# the catalog, schema, and registered model name\n",
    "# in Unity Catalog under which to create the version\n",
    "# The registered model will be created if it doesn't already exist\n",
    "autolog_run = mlflow.last_active_run()\n",
    "model_uri = \"runs:/{}/model\".format(autolog_run.info.run_id)  # NOTE: Can this be automatic?\n",
    "mlflow.register_model(model_uri, \"aza-databricks-b9b7aae-catalog.rocketfuel.iris_model\")  # NOTE: Can this be automatic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f0bc9c-1ccd-4dee-b8c4-d4f24305eaac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "session.run(\"__default__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae964514-9785-4599-b8dc-bed9f8159489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register models using the Databricks Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note**: fsspec uses the DBFS API, which is not compatible with Unity Catalog according to https://github.com/fsspec/filesystem_spec/issues/1656_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5924746010652874,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_2-Kedro-on-Databricks-part-2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
