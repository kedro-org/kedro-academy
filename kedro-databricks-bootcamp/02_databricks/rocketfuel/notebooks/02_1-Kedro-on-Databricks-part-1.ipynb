{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0efbb1e-7e70-4e3e-b322-7abafa55d055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02.1 - Kedro on Databricks, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11bb477b-f92a-443b-b731-834e352ffbbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Overview of deployment patterns\n",
    "\n",
    "- Day 1: explore Kedro locally (with Pandas instead of PySpark)\n",
    "- Day 2, part 1: explore Kedro fully on Databricks\n",
    "- Day 2, part 2: develop Kedro locally, execute on Databricks (with `databricks-connect`)\n",
    "- Day 3: intermediate topics, including Databricks Asset Bundles (with `kedro-databricks`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba8a9dd-5a3a-424a-bbca-0616dfe776e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Workspace notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd012e3c-6560-4c02-92a2-569e88634f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Kedro and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb14042-e923-442e-b6ed-dc2c118958e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../../requirements.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7980ed0-13c6-4818-9581-69f27ff27296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install hdfs s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd50f264-b5cf-4b44-919c-aa9e2a9ace9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd95abb2-17ae-480c-8547-a20700e6ae35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load data from workspace files using `spark.SparkDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70fb0484-8e9e-4407-9141-15dd58f8aa2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading data directly from the workspace may be disabled depending on the cluster configuration.\n",
    "\n",
    "First verify that the data is available from a Volume in Unity Catalog first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d9fa33-d29f-45cd-9cc0-9da9d190aacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/bronze/companies.csv\").show(5)  # Replace with your catalog, schema, and volume!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a Kedro catalog to load the data from the Databricks workspace. We'll define the data as a `spark.SparkDataset` in the catalog, which allows us to load the data as a PySpark DataFrame."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9975c81-a72d-4ccc-9971-48eea010db25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kedro.io import DataCatalog\n",
    "\n",
    "interactive_catalog = DataCatalog.from_config(\n",
    "    {\n",
    "        \"companies_raw\": {\n",
    "            \"type\": \"spark.SparkDataset\",\n",
    "            \"filepath\": \"/Volumes/aza-databricks-b9b7aae-catalog/rocketfuel/bronze/companies.csv\",\n",
    "            \"file_format\": \"csv\",\n",
    "            \"load_args\": {\n",
    "                \"header\": True,\n",
    "                \"inferSchema\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "interactive_catalog.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275f5ce5-17e8-4c9a-95fa-1e257f38639b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(interactive_catalog.load(\"companies_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create configuration files to use the Kedro config loader to load data from the workspace files. First we'll create a databricks environment inside the configuration directory to hold the configuration files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa6df46-ef1d-4cca-90df-f164b19fb3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p ../conf/databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Kedro catalog yaml file to load the data from the Databricks workspace.\n",
    "\n",
    "Don't forget to update the `_uc_catalog` and `_uc_schema` variables to match your cluster and unity catalog configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4ac5c2-a5d0-4189-8049-33670b98a2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ../conf/databricks/catalog.yml\n",
    "_uc_catalog: aza-databricks-b9b7aae-catalog\n",
    "_uc_schema: rocketfuel\n",
    "\n",
    "companies_raw:\n",
    "  type: spark.SparkDataset\n",
    "  filepath: /Volumes/${_uc_catalog}/${_uc_schema}/bronze/companies.csv\n",
    "  file_format: csv\n",
    "  load_args:\n",
    "    header: True\n",
    "    inferSchema: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a config loader that uses the databricks environment as the default run environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7236fb28-eec8-40da-b650-4394d480d3e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kedro.config import OmegaConfigLoader\n",
    "\n",
    "config_loader = OmegaConfigLoader(\n",
    "    conf_source=\"../conf\",\n",
    "    base_env=\"base\",\n",
    "    default_run_env=\"databricks\",  # Notice newly created environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Kedro catalog object using the config loader:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "catalog_config = config_loader.get(\"catalog\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Check the data can be loaded from the catalog configuration:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22dd2ff3-f8cb-4336-852a-26525ee0c3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "catalog_config[\"companies_raw\"]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And finally create a Kedro `DataCatalog` object from the configuration:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d759df-ab8a-44f6-a4f5-1bc57a6de185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "interactive_catalog = DataCatalog.from_config(catalog_config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Fetch and display the data:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6036842c-7f89-4413-a405-2804edd33a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(interactive_catalog.load(\"companies_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f746f72-0a40-4a28-be79-a45c73950b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bootstrap the Kedro project inside a Databricks notebook using the Kedro extension for IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb49610-ccf9-40be-99b0-deacbc2b2687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# XXX: Shouldn't be necessary, but DISABLE_HOOKS_FOR_PLUGINS seems to have no effect\n",
    "%pip uninstall -y kedro-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way to explore the Kedro `catalog`, `context`, `pipelines`, and `session` variables in your project within a IPython compatible environment, such as Databricks notebooks, Google Colab, and more, is to use the `kedro.ipython` extension. This is tool-independent and useful in situations where launching a Jupyter interactive environment is not possible. You can use the `%load_ext` line magic to explicitly load the Kedro IPython extension:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae5cb42-f391-4b6e-979e-e69273a913a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `%reload_kedro` line magic within your Jupyter notebook to reload the Kedro variables (for example, if you need to update `catalog` following changes to your Data Catalog).\n",
    "\n",
    "You don’t need to restart the kernel for the `catalog`, `context`, `pipelines` and `session` variables.\n",
    "\n",
    "%reload_kedro accepts optional keyword arguments env and params. For example, to use configuration environment `databricks`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2239fa-8ca6-43f3-b2e5-ea6ca2941620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_kedro --env databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can load the project catalog and fetch data from it. Note that this is a different catalog object from the one created earlier as this is created from the Kedro project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbdff0c-45e6-4e96-84ed-81253dcc3bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog.list(\".*_raw\")  # Accepts a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df30e38-234d-4ce5-b629-bf2b3461bd8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(catalog.load(\"companies_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e649a72-95de-439a-86ba-cbbe272ad93b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1 (5 mins)\n",
    "\n",
    "Complete the `catalog.yml` of the `databricks` environment inside the `rocketfuel` project by adding a new datasets:\n",
    "\n",
    "- `reviews_raw` using `spark.SparkDataset` (similar configuration as `companies_raw`)\n",
    "\n",
    "When done, reload the Kedro project and verify that loading the data works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a9530b-7fcd-4bfe-b6f3-00c9d2c0ff84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Connection with the Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d4840e5-397d-433f-8a3c-19dfb69896c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Normally we will load all the structured data from Unity Catalog tables.\n",
    "\n",
    "Therefore, let's write a first pipeline that ingests these CSV and Excel files into the UC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b972e657-adea-4673-aacb-1c09a99bf820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read and write structured data to Databricks Unity Catalog using `databricks.ManagedTableDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b1b686f-89c1-4c55-af85-33202dbd1801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "Add a new dataset entry for the `companies` dataset that uses the `databricks.ManagedTableDataset` type. This dataset type allows you to write and load data to a Unity Catalog table in Databricks."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa31f4c-242c-42bf-b871-a25581211aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a ../conf/databricks/catalog.yml\n",
    "companies:\n",
    "  type: databricks.ManagedTableDataset\n",
    "  catalog: ${_uc_catalog}\n",
    "  database: ${_uc_schema}\n",
    "  table: companies\n",
    "  write_mode: overwrite"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reload the `kedro.ipython` extension and project with the updated catalog."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827f3b5d-e1f6-4cc3-8315-1dfbf5267232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41874df4-c475-4937-b46f-05158eea0366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_kedro --env databricks"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the raw data again from the Databricks workspace."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5759463f-cb5d-40b4-9aed-9842d24a9e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_companies = catalog.load(\"companies_raw\")\n",
    "df_companies.show(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now save this data into the Unity Catalog table using the `databricks.ManagedTableDataset` dataset type. "
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b9650d-5a34-4331-96d7-63cee208b5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog.save(\"companies\", df_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "047de5c1-92a7-4532-b657-a98cdc3393a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment if cell below does not show any output,\n",
    "# see https://github.com/kedro-org/kedro/issues/4804\n",
    "# %sh uv pip uninstall rich"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Verify you can read the data from the Unity Catalog table using a SQL command. This will also verify that the table was created successfully."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9984a13-1d3a-4b91-98fa-a8526c258a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM `aza-databricks-b9b7aae-catalog`.rocketfuel.companies\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7991d56-2570-41fb-baa3-de189af06350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2 (5 mins)\n",
    "\n",
    "Add a new datasets to `conf/databricks/catalog.yml` to represent the Delta Tables of `reviews`,\n",
    "and ingest the data manually from the notebook.\n",
    "\n",
    "When you are done, run the appropriate SQL command to verify that everything worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The full documentation for the Kedro `databricks.ManagedTableDataset` is available [here](https://docs.kedro.org/projects/kedro-datasets/en/kedro-datasets-7.0.0/api/kedro_datasets.databricks.ManagedTableDataset.html).\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 3 (15 mins)\n",
    "\n",
    "Codify the logic to load the raw data into the Unity Catalog in a `load_data` Kedro pipeline. For that:\n",
    "\n",
    "- Create a function to load and return a spark dataframe to be used to load the raw companies and reviews data.\n",
    "- Create two Kedro nodes to load each dataset into the Unity Catalog.\n",
    "- Create a Kedro pipeline from these two nodes.\n",
    "- Use the NotebookVisualizer to visualise the pipeline in the notebook."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kedro.pipeline import node, pipeline\n",
    "\n",
    "### Your code goes here\n",
    "\n",
    "# node1 = node(func=..., inputs=\"companies_raw\", outputs=\"companies\")\n",
    "\n",
    "# load_data = pipeline([node1, node2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513046d3-dc74-41b3-8581-fcc04bd03b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kedro_viz.integrations.notebook import NotebookVisualizer\n",
    "\n",
    "NotebookVisualizer(load_data).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5632963819948557,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_1-Kedro-on-Databricks-part-1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
