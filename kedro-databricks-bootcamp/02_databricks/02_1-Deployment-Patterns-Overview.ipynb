{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd71a91-f200-4a80-ac38-58063c0624ce",
   "metadata": {},
   "source": [
    "# Overview of Deployment Patterns while working with Databricks\n",
    "\n",
    "Databricks offers integration with Kedro through three principal workflows\n",
    "1. Work within Databricks workspace\n",
    "2. Hybrid workflow combining local IDE with Databricks\n",
    "3. Deploy a packaged Kedro project to Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef4e1f-e690-401d-b89f-d73217e69311",
   "metadata": {},
   "source": [
    "## Work within Databricks workspace\n",
    "\n",
    "<b>Pros & Cons:</b>\n",
    "- Ideal for developers who prefer developing their projects in notebooks rather than an in an IDE\n",
    "- Avoids the overhead of setting up and syncing a local environment with Databricks\n",
    "- Flexibility for quick iteration\n",
    "- But for production deployment, you need to consider a job-based deployment workflow\n",
    "\n",
    "<b>Prerequisites:</b>\n",
    "- An active Databricks account\n",
    "- A Databricks cluster configured with a recent version (>= 11.3 is recommended) of the Databricks runtime.\n",
    "- Python >= 3.9 installed.\n",
    "- A GitHub account.\n",
    "- Git installed.\n",
    "- A python env management system installed, venv or conda are popular choices.\n",
    "\n",
    "<b>Steps:</b>\n",
    "1. On your machine, Install Kedro in a new virtual env and create a new Kedro project\n",
    "2. Create a GitHub repo and push your Kedro project to the repo\n",
    "3. Create a git repo on Databricks linking your Kedro project from step2\n",
    "4. Create a new Databricks notebook\n",
    "5. On Databricks, Kedro cannot access data stored directly in your project’s directory. As a result, you’ll need to move your \n",
    "project’s data to a location accessible. Copying files to DBFS or Volumes\n",
    "6. Install project requirements - `%pip install -r \"/Workspace/Repos/<databricks_username>/<project-name>/requirements.txt\"`\n",
    "7. Load Kedro IPython extension - `%load_ext kedro.ipython`\n",
    "8. Load your Kedro project - `%reload_kedro /Workspace/Repos/<databricks_username>/<project-name>`. Loading your Kedro project \n",
    "with the `%reload_kedro` line magic will define four global variables in your notebook: context, session, catalog and pipelines. \n",
    "You will use the `session` variable to run your project.\n",
    "9. Run your Kedro project - `session.run()`. You can modify and re-run the Kedro project, sync it with the remote GitHub repo. \n",
    "\n",
    "<b>Reference</b>\n",
    "https://docs.kedro.org/en/stable/deployment/databricks/databricks_notebooks_development_workflow.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e537be-97b3-4c06-9555-c9c601b764d7",
   "metadata": {},
   "source": [
    "## Hybrid workflow combining local IDE with Databricks\n",
    "\n",
    "<b>Pros & Cons:</b>\n",
    "- IDE’s capabilities for faster, error-free development, while testing on Databricks\n",
    "- Helps with constant adjustments during early stages of learning Kedro\n",
    "- Auto-completion and suggestions for code, improving your development speed and accuracy\n",
    "- Linters like Ruff can be integrated to catch potential issues in your code.\n",
    "- Static type checkers like Mypy can check types in your code, helping to identify potential type-related issues \n",
    "early in the development process.\n",
    "- Use Databricks connect and run your project with Databricks compute.\n",
    "- Use `kedro-databricks` and Databricks Asset Bundle to package your code for running pipelines on Databricks.\n",
    "- But for production deployment, you need to consider a job-based deployment workflow\n",
    "\n",
    "<b>Prerequisites:</b>\n",
    "- An active Databricks account\n",
    "- A Databricks cluster configured with a recent version (>= 11.3 is recommended) of the Databricks runtime.\n",
    "- Python >= 3.9 installed.\n",
    "- A python env management system installed, venv or conda are popular choices.\n",
    "\n",
    "<b>Steps:</b>\n",
    "1. On your machine, Install Kedro in a new virtual env and create a new Kedro project\n",
    "2. Install and Authenticate the Databricks CLI - `pip install databricks-cli` and `databricks configure --token`. \n",
    "You need to get Databricks host and create a personal access token to provide when prompted.\n",
    "3. Iterate and develop your Kedro project as per your requirements. \n",
    "4. Run your kedro project either via local compute or Databricks compute (using Databricks Connect)\n",
    "5. Create Databricks Asset Bundles using `kedro-databricks` - `pip install kedro-databricks`, `kedro databricks init` \n",
    "and `kedro databricks bundle` which creates a Databricks job configuration inside `resource` folder\n",
    "6. Deploy Databricks job using Databricks Asset Bundles - `kedro databricks deploy`\n",
    "7. Run Databricks Job with databricks CLI - databricks bundle run\n",
    "\n",
    "<b>Reference</b>\n",
    "https://docs.kedro.org/en/stable/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5be8a2-827a-4c21-bec5-46e40563ce5b",
   "metadata": {},
   "source": [
    "## Deploy a packaged Kedro project to Databricks\n",
    "\n",
    "<b>Pros & Cons:</b>\n",
    "- Go to choice when dealing with complex project requirements and production ready pipelines\n",
    "- Provides a structured and reproducible way to run your code\n",
    "- Significantly slower than running it as a notebook on a cluster that has already been started\n",
    "- No way to change your project’s code once it has been packaged\n",
    "- Unsuitable for development projects where rapid iteration is necessary\n",
    "\n",
    "<b>Prerequisites:</b>\n",
    "- An active Databricks account\n",
    "- A Databricks cluster configured with a recent version (>= 11.3 is recommended) of the Databricks runtime.\n",
    "- Python >= 3.9 installed.\n",
    "- A python env management system installed, venv or conda are popular choices.\n",
    "    \n",
    "<b>Steps:</b>\n",
    "1. On your machine, Install Kedro in a new virtual env and create a new Kedro project\n",
    "2. Install and Authenticate the Databricks CLI - `pip install databricks-cli` and `databricks configure --token`. \n",
    "You need to get Databricks host and create a personal access token to provide when prompted.\n",
    "3. Iterate and develop your Kedro project as per your requirements. \n",
    "4. Run your kedro project either via local compute or Databricks compute (using Databricks Connect)\n",
    "5. Package your project - `kedro package`, This command generates a .whl file in the dist directory within your project’s root directory.\n",
    "6. Upload project data and configuration to DBFS or Volumes - A Kedro project’s configuration and data do not get included when it is packaged. \n",
    "They must be stored somewhere accessible to allow your packaged project to run.\n",
    "7. Deploy and run your Kedro project using the Workspace UI - \n",
    "https://docs.kedro.org/en/stable/deployment/databricks/databricks_deployment_workflow.html#deploy-and-run-your-kedro-project-using-the-workspace-ui\n",
    "\n",
    "<b>Reference</b>\n",
    "https://docs.kedro.org/en/stable/deployment/databricks/databricks_deployment_workflow.html#use-a-databricks-job-to-deploy-a-kedro-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410e1fb-6f6c-4add-abd2-0fa602dd9b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
